{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_dataset = pd.read_csv(r'F:\\std\\allreplies.csv')\n",
    "replies_dataset = replies_dataset.head(350)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset which is scraped from 'American Sexual Health Association(ASHA)' on which the Topic Modelling model is to be built is loaded into a pandas dataframe.The data set contains user posts and replies for various sexually transmitted diseases. We will use LDA to group the replies into 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>REPLIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Dear members,\\n\\nMany of us, including myself,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yarnkitty</td>\n",
       "      <td>As a former nurse and too frequent patient, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bb45694</td>\n",
       "      <td>I think security should be tighter.\\nOne thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hbc2115</td>\n",
       "      <td>Throughout my surgeries and chemo, I had very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queencitywalker</td>\n",
       "      <td>Patient centered, not dollar centered.\\nSmalle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                            REPLIES\n",
       "0            Brian  Dear members,\\n\\nMany of us, including myself,...\n",
       "1        yarnkitty  As a former nurse and too frequent patient, my...\n",
       "2          bb45694  I think security should be tighter.\\nOne thing...\n",
       "3          Hbc2115  Throughout my surgeries and chemo, I had very ...\n",
       "4  queencitywalker  Patient centered, not dollar centered.\\nSmalle..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies_dataset.dropna()\n",
    "replies_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"http\", \" \", case = False) \n",
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"www\", \" \", case = False) \n",
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"org\", \" \", case = False) \n",
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"https\", \" \", case = False)\n",
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"ashasexualhealth\", \" \", case = False)\n",
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"thanks\", \" \", case = False)\n",
    "replies_dataset['REPLIES']= replies_dataset['REPLIES'].str.replace(\"don\", \" \", case = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the frequently repeating words which does not add value to the topics are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(replies_dataset['REPLIES'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the CountVectorizer class from the sklearn.feature_extraction.text module to create a document-term matrix. We specify to only include those words that appear in less than 80% of the document and appear in at least 2 documents. We also remove all the stop words as they do not really contribute to topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our document term matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<350x1717 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10285 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary has 10285 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use LDA to create topics along with the probability distribution for each word in our vocabulary for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the LatentDirichletAllocation class from the sklearn.decomposition library to perform LDA \n",
    "on our document-term matrix. The parameter n_components specifies the number of categories, or topics,that we want our text \n",
    "to be divided into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script randomly fetches 10 words from our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal\n",
      "shoes\n",
      "wrote\n",
      "influence\n",
      "diabetes\n",
      "preparing\n",
      "marriage\n",
      "empathic\n",
      "unless\n",
      "benefit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])\n",
    "\n",
    "first_topic = LDA.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find 10 words with the highest probability for the first topic. To get the first topic, we can use the\n",
    "components_ attribute and pass a 0 index as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients\n",
      "patient\n",
      "ve\n",
      "doctor\n",
      "symptoms\n",
      "think\n",
      "people\n",
      "like\n",
      "just\n",
      "know\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indexes can then be used to retrieve the value of the words from the count_vect object, which can be done like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients\n",
      "patient\n",
      "ve\n",
      "doctor\n",
      "symptoms\n",
      "think\n",
      "people\n",
      "like\n",
      "just\n",
      "know\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words show that the first topic might be about herpes and feeling low due to its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the 10 words with highest probabilities for all the five topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['patients', 'patient', 've', 'doctor', 'symptoms', 'think', 'people', 'like', 'just', 'know']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['let', 'herpes', 'know', 'admin', 'post', 'read', 'com', 'hsv', 'health', 'inspire']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['inspire', 'treatment', 'think', 'link', 'information', 'medical', 'years', 'know', 'right', 'just']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['people', 'want', 'sex', 'feel', 'just', 'child', 'like', 'hsv', 'life', 'herpes']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['really', 'know', 'testing', 'hi', 'time', 'dormant', 'people', 'thank', 'hpv', 'nan']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 :Discussions about herpes (most prevelant disease)<br>\n",
    "Topic 1 :Discussions about HPV(human papillomavirus)<br>\n",
    "Topic 2 :Discussions on transmission mode of the herpes virus<br>\n",
    "Topic 3 :Discussions about the forum(Inspire)(American Sexual health association)<br>\n",
    "Topic 4 :Discussions about doctors,patients and treatments\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we will add a column to the original data frame that will store the topic for the text. To do so, we can use\n",
    "LDA.transform() method and pass it our document-term matrix. This method will assign the probability of all the topics to \n",
    "each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script adds a new column for topic in the data frame and assigns the topic value to each row in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_dataset['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_dataset.to_csv(r'F:\\std\\topic_all_replies_LDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>REPLIES</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Dear members,\\n\\nMany of us, including myself,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yarnkitty</td>\n",
       "      <td>As a former nurse and too frequent patient, my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bb45694</td>\n",
       "      <td>I think security should be tighter.\\nOne thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hbc2115</td>\n",
       "      <td>Throughout my surgeries and chemo, I had very ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queencitywalker</td>\n",
       "      <td>Patient centered, not dollar centered.\\nSmalle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                            REPLIES  Topic\n",
       "0            Brian  Dear members,\\n\\nMany of us, including myself,...      0\n",
       "1        yarnkitty  As a former nurse and too frequent patient, my...      0\n",
       "2          bb45694  I think security should be tighter.\\nOne thing...      0\n",
       "3          Hbc2115  Throughout my surgeries and chemo, I had very ...      0\n",
       "4  queencitywalker  Patient centered, not dollar centered.\\nSmalle...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies_dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
