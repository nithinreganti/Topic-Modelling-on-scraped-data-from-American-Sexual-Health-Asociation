{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "height": 200,
       "width": 500
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"F:\\std\\logo.png\", width=500, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "American Sexual Health Association (ASHA): ASHA is a forum where people discuss their sexual concerns. A lot of posts here go unreplied and also people who interact here are the ones who are afraid of approaching a doctor .\n",
    "\n",
    "The link to the website : https://www.inspire.com/groups/american-sexual-health-association/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "To increase the patient doctor interaction , the doctors wanted to know about what was discussed in the forum so that they can approach the patients .\n",
    "\n",
    "Reading the whole portal manually is humanly impossible and a very tedious job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "The solution to such a problem is by using the techniques of Natural language processing . One such algorithm which can explain the major topics in the huge set of words is Topic Modeling. To build this model we need to have a text dataset .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:  \n",
    "The entire forum is scraped using web scraping libraries like selenium and beautiful soup. Each post leads to the full description which further leads to Bio information of each member .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Information gathered are:\n",
    "1. Username\n",
    "2. Profile URL\n",
    "3. Time of post\n",
    "4. Replies\n",
    "5. Bio data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling using LDA (Data Source: American sexual health association )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset which is scraped from 'American Sexual Health Association(ASHA)' on which the Topic Modelling model is to be built is loaded into a pandas dataframe.The data set contains user posts and replies for various sexually transmitted diseases. We will use LDA to group the replies into 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = pd.read_csv(r'F:\\std\\allreplies.csv')\n",
    "replies = replies.head(3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Post and replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Dear members,\\n\\nMany of us, including myself,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yarnkitty</td>\n",
       "      <td>As a former nurse and too frequent patient, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bb45694</td>\n",
       "      <td>I think security should be tighter.\\nOne thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hbc2115</td>\n",
       "      <td>Throughout my surgeries and chemo, I had very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queencitywalker</td>\n",
       "      <td>Patient centered, not dollar centered.\\nSmalle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Username                                    Post and replies\n",
       "0            Brian  Dear members,\\n\\nMany of us, including myself,...\n",
       "1        yarnkitty  As a former nurse and too frequent patient, my...\n",
       "2          bb45694  I think security should be tighter.\\nOne thing...\n",
       "3          Hbc2115  Throughout my surgeries and chemo, I had very ...\n",
       "4  queencitywalker  Patient centered, not dollar centered.\\nSmalle..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.dropna()\n",
    "replies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['Post and replies'][350]\n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"http\", \" \", case = False) \n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"www\", \" \", case = False) \n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"org\", \" \", case = False) \n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"https\", \" \", case = False)\n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"ashasexualhealth\", \" \", case = False)\n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"thanks\", \" \", case = False)\n",
    "replies['Post and replies']= replies['Post and replies'].str.replace(\"don\", \" \", case = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the frequently repeating words which does not add value to the topics are removed.\n",
    "### Creating a  vocabulary of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(replies['Post and replies'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the CountVectorizer class from the sklearn.feature_extraction.text module to create a document-term matrix. We specify to only include those words that appear in less than 80% of the document and appear in at least 2 documents. We also remove all the stop words as they do not really contribute to topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our document term matrix\n",
    "### Document Term matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3500x6708 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 101797 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary has 101797 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LDA to create topics\n",
    "Next, we will use LDA to create topics along with the probability distribution for each word in our vocabulary for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the LatentDirichletAllocation class from the sklearn.decomposition library to perform LDA \n",
    "on our document-term matrix. The parameter n_components specifies the number of categories, or topics,that we want our text \n",
    "to be divided into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Random words from our Vocabulary\n",
    "The following script randomly fetches 10 words from our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "lets\n",
      "esteem\n",
      "friendship\n",
      "accounts\n",
      "anxiety\n",
      "awhile\n",
      "hose\n",
      "untreated\n",
      "estimate\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])\n",
    "\n",
    "first_topic = LDA.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find 10 words with the highest probability for the first topic. To get the first topic, we can use the\n",
    "components_ attribute and pass a 0 index as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did\n",
      "time\n",
      "know\n",
      "told\n",
      "feel\n",
      "herpes\n",
      "life\n",
      "just\n",
      "like\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indexes can then be used to retrieve the value of the words from the count_vect object, which can be done like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words show that the first topic might be about herpes and feeling low due to its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding words with highest probabilities in all the topics \n",
    "Let's print the 10 words with highest probabilities for all the five topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['did', 'time', 'know', 'told', 'feel', 'herpes', 'life', 'just', 'like', 'nan']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['ve', 'symptoms', 'years', 'people', 'like', 'time', 'test', 'just', 'know', 'hpv']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['health', 'transmission', 'check', 'sex', 'stdsstis', 'oral', 'genital', 'risk', 'hsv', 'herpes']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['cancer', 'know', 'support', 'members', 'post', 'just', 'people', 'like', 'thank', 'inspire']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['new', 'dr', 'treatment', 'pain', 'care', 'doctors', 'doctor', 'patients', 'medical', 'patient']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Description\n",
    "Topic 0 :Discussions about herpes (most prevelant disease)<br>\n",
    "Topic 1 :Discussions about HPV(human papillomavirus)<br>\n",
    "Topic 2 :Discussions on transmission mode of the herpes virus<br>\n",
    "Topic 3 :Discussions about the forum(Inspire)(American Sexual health association)<br>\n",
    "Topic 4 :Discussions about doctors,patients and treatments\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we will add a column to the original data frame that will store the topic for the text. To do so, we can use\n",
    "LDA.transform() method and pass it our document-term matrix. This method will assign the probability of all the topics to \n",
    "each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script adds a new column for topic in the data frame and assigns the topic value to each row in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies.to_csv(r'F:\\std\\topic_all_replies_LDA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics assigned to each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Post and replies</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Dear members,\\n\\nMany of us, including myself,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yarnkitty</td>\n",
       "      <td>As a former nurse and too frequent patient, my...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bb45694</td>\n",
       "      <td>I think security should be tighter.\\nOne thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hbc2115</td>\n",
       "      <td>Throughout my surgeries and chemo, I had very ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queencitywalker</td>\n",
       "      <td>Patient centered, not dollar centered.\\nSmalle...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Username                                    Post and replies  Topic\n",
       "0            Brian  Dear members,\\n\\nMany of us, including myself,...      3\n",
       "1        yarnkitty  As a former nurse and too frequent patient, my...      4\n",
       "2          bb45694  I think security should be tighter.\\nOne thing...      0\n",
       "3          Hbc2115  Throughout my surgeries and chemo, I had very ...      4\n",
       "4  queencitywalker  Patient centered, not dollar centered.\\nSmalle...      4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
